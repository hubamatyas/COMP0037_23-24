Sarsa is on-policy because we sample the next action directly from our (target) policy, $A_{t+1} = \pi(S_{t+1})$,
meaning the action the agent takes 