The off-policy MC policy prediction algorithm balances exploitation and exploration by
using two policies, the target policy $\pi(A|s)$ and the behaviour policy $b(A|S)$. 
The target policy is the policy we are interested in optimising and what the agent runs after training, while the behaviour 
policy is used to generate behaviour of the agent in each episode. To ensure
there is sufficient data on each state-action pair and that the policy doesn't \textbf{freeze},
$\pi(A|S)$ must be soft, for example, $\episilon$-greedy, meaning there is a non-zero probability
of selecting any action in any state. On the other hand, $\pi(A|S)$ can be deterministic and use
the greedy policy to select actions (i.e., $argmax_a Q(S, a)$).

However, we must also ensure the target policy is evaluated correctly, the behaviour policy must cover the
target policy, i.e. $b(A|S) > 0$ whenever $\pi(A|S) > 0$.
